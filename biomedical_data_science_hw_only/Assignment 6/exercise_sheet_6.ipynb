{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c980becf",
   "metadata": {},
   "source": [
    "## Exercise sheet 6\n",
    "\n",
    "Cüneyt Erem\n",
    "3277992\n",
    "s6cuerem@uni-bonn.de\n",
    "\n",
    "Nkeh Victor Ndiwago\n",
    "3504121\n",
    "s0vinkeh@uni-bonn.de\n",
    "\n",
    "Paula Romero Jiménez\n",
    "3320220\n",
    "s0parome@uni-bonn.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c545a0d",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335f1ef",
   "metadata": {},
   "source": [
    "Exercise 1 - NMF Clustering (17 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60935eb",
   "metadata": {},
   "source": [
    "1. Investigate the given gene expression (GE) dataset and you are asked to cluster \n",
    "the patients/samples based on their GE profile.\n",
    "\n",
    "a. Which unsupervised ML algorithm would you suggest to tackle this task? (1 \n",
    "point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82bb3c5",
   "metadata": {},
   "source": [
    "- Agglomerative hierarchical clustering algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a6bc3",
   "metadata": {},
   "source": [
    "b. Briefly explain the reasoning behind your suggestion. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58526ea5",
   "metadata": {},
   "source": [
    "The objective of any agglomerative hierarchical clustering algorithm is to cluster a set of n objects (e.g., patients, genes) based on an n × n similarity matrix. The reason for choosing this clustering algorithm is due to its ability to simultaneously discover several layers of clustering structure, and visualize these layers via tree diagrams (i.e., dendrogram)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c45b73",
   "metadata": {},
   "source": [
    "2. Using the nimfa library, carry out NMF on GE data.\n",
    "\n",
    "a. Fit the NMF model in the nimfa library to the GE data and using the \n",
    "cophenetic correlation, estimate the number of clusters (rank) needed to \n",
    "classify the samples in the GE data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c84325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>36638_at</th>\n",
       "      <th>39318_at</th>\n",
       "      <th>38514_at</th>\n",
       "      <th>266_s_at</th>\n",
       "      <th>38585_at</th>\n",
       "      <th>41266_at</th>\n",
       "      <th>36108_at</th>\n",
       "      <th>39389_at</th>\n",
       "      <th>31525_s_at</th>\n",
       "      <th>...</th>\n",
       "      <th>37655_at</th>\n",
       "      <th>1440_s_at</th>\n",
       "      <th>32260_at</th>\n",
       "      <th>40070_at</th>\n",
       "      <th>1056_s_at</th>\n",
       "      <th>39200_s_at</th>\n",
       "      <th>36105_at</th>\n",
       "      <th>32578_at</th>\n",
       "      <th>39383_at</th>\n",
       "      <th>33718_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01005</td>\n",
       "      <td>0.583368</td>\n",
       "      <td>0.535258</td>\n",
       "      <td>0.642984</td>\n",
       "      <td>0.891901</td>\n",
       "      <td>0.269871</td>\n",
       "      <td>0.605566</td>\n",
       "      <td>0.069070</td>\n",
       "      <td>0.938101</td>\n",
       "      <td>0.314737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884590</td>\n",
       "      <td>0.399443</td>\n",
       "      <td>0.379796</td>\n",
       "      <td>0.272379</td>\n",
       "      <td>0.216088</td>\n",
       "      <td>0.355740</td>\n",
       "      <td>0.167203</td>\n",
       "      <td>0.438651</td>\n",
       "      <td>0.395720</td>\n",
       "      <td>0.273593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01010</td>\n",
       "      <td>0.505321</td>\n",
       "      <td>0.704177</td>\n",
       "      <td>0.913612</td>\n",
       "      <td>0.657634</td>\n",
       "      <td>0.402911</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>0.803187</td>\n",
       "      <td>0.360471</td>\n",
       "      <td>0.718665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086277</td>\n",
       "      <td>0.432789</td>\n",
       "      <td>0.431440</td>\n",
       "      <td>0.461242</td>\n",
       "      <td>0.443493</td>\n",
       "      <td>0.291054</td>\n",
       "      <td>0.102883</td>\n",
       "      <td>0.584035</td>\n",
       "      <td>0.589994</td>\n",
       "      <td>0.401375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03002</td>\n",
       "      <td>0.375805</td>\n",
       "      <td>0.073716</td>\n",
       "      <td>0.707562</td>\n",
       "      <td>0.847162</td>\n",
       "      <td>0.792428</td>\n",
       "      <td>0.819212</td>\n",
       "      <td>0.644334</td>\n",
       "      <td>0.735292</td>\n",
       "      <td>0.828776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750758</td>\n",
       "      <td>0.305592</td>\n",
       "      <td>0.444996</td>\n",
       "      <td>0.293573</td>\n",
       "      <td>0.318939</td>\n",
       "      <td>0.259713</td>\n",
       "      <td>0.271869</td>\n",
       "      <td>0.556874</td>\n",
       "      <td>0.931118</td>\n",
       "      <td>0.301529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.226960</td>\n",
       "      <td>0.119596</td>\n",
       "      <td>0.394317</td>\n",
       "      <td>0.115411</td>\n",
       "      <td>0.050117</td>\n",
       "      <td>0.440698</td>\n",
       "      <td>0.649291</td>\n",
       "      <td>0.498758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090901</td>\n",
       "      <td>0.365967</td>\n",
       "      <td>0.170796</td>\n",
       "      <td>0.565408</td>\n",
       "      <td>0.456499</td>\n",
       "      <td>0.407845</td>\n",
       "      <td>0.100038</td>\n",
       "      <td>0.670769</td>\n",
       "      <td>0.541601</td>\n",
       "      <td>0.659517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04007</td>\n",
       "      <td>0.890125</td>\n",
       "      <td>0.631314</td>\n",
       "      <td>0.518785</td>\n",
       "      <td>0.880312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858408</td>\n",
       "      <td>0.638519</td>\n",
       "      <td>0.933899</td>\n",
       "      <td>0.963169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340640</td>\n",
       "      <td>0.285412</td>\n",
       "      <td>0.561193</td>\n",
       "      <td>0.470224</td>\n",
       "      <td>0.468927</td>\n",
       "      <td>0.243284</td>\n",
       "      <td>0.467118</td>\n",
       "      <td>0.571348</td>\n",
       "      <td>0.425701</td>\n",
       "      <td>0.196066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>56007</td>\n",
       "      <td>0.072759</td>\n",
       "      <td>0.068235</td>\n",
       "      <td>0.473041</td>\n",
       "      <td>0.239455</td>\n",
       "      <td>0.223252</td>\n",
       "      <td>0.280791</td>\n",
       "      <td>0.082024</td>\n",
       "      <td>0.133053</td>\n",
       "      <td>0.768916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371955</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.415382</td>\n",
       "      <td>0.351072</td>\n",
       "      <td>0.351224</td>\n",
       "      <td>0.513185</td>\n",
       "      <td>0.136617</td>\n",
       "      <td>0.558943</td>\n",
       "      <td>0.225453</td>\n",
       "      <td>0.291071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>64005</td>\n",
       "      <td>0.053972</td>\n",
       "      <td>0.086147</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.136057</td>\n",
       "      <td>0.488365</td>\n",
       "      <td>0.470852</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>0.116930</td>\n",
       "      <td>0.861130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359889</td>\n",
       "      <td>0.557229</td>\n",
       "      <td>0.388962</td>\n",
       "      <td>0.375595</td>\n",
       "      <td>0.298626</td>\n",
       "      <td>0.584230</td>\n",
       "      <td>0.079315</td>\n",
       "      <td>0.353649</td>\n",
       "      <td>0.185538</td>\n",
       "      <td>0.461440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>65003</td>\n",
       "      <td>0.136163</td>\n",
       "      <td>0.082575</td>\n",
       "      <td>0.456977</td>\n",
       "      <td>0.120866</td>\n",
       "      <td>0.216307</td>\n",
       "      <td>0.239017</td>\n",
       "      <td>0.069328</td>\n",
       "      <td>0.164105</td>\n",
       "      <td>0.565675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720453</td>\n",
       "      <td>0.632587</td>\n",
       "      <td>0.652855</td>\n",
       "      <td>0.336831</td>\n",
       "      <td>0.153023</td>\n",
       "      <td>0.757112</td>\n",
       "      <td>0.179819</td>\n",
       "      <td>0.494060</td>\n",
       "      <td>0.277194</td>\n",
       "      <td>0.224542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>83001</td>\n",
       "      <td>0.022913</td>\n",
       "      <td>0.026321</td>\n",
       "      <td>0.774337</td>\n",
       "      <td>0.135344</td>\n",
       "      <td>0.379465</td>\n",
       "      <td>0.544207</td>\n",
       "      <td>0.049219</td>\n",
       "      <td>0.015351</td>\n",
       "      <td>0.724584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768953</td>\n",
       "      <td>0.357137</td>\n",
       "      <td>0.396453</td>\n",
       "      <td>0.674790</td>\n",
       "      <td>0.207006</td>\n",
       "      <td>0.287185</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.327525</td>\n",
       "      <td>0.157277</td>\n",
       "      <td>0.337916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>LAL4</td>\n",
       "      <td>0.060686</td>\n",
       "      <td>0.091866</td>\n",
       "      <td>0.710210</td>\n",
       "      <td>0.218240</td>\n",
       "      <td>0.302652</td>\n",
       "      <td>0.624061</td>\n",
       "      <td>0.251337</td>\n",
       "      <td>0.037572</td>\n",
       "      <td>0.452716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528329</td>\n",
       "      <td>0.787925</td>\n",
       "      <td>0.278878</td>\n",
       "      <td>0.231955</td>\n",
       "      <td>0.233604</td>\n",
       "      <td>0.869236</td>\n",
       "      <td>0.174480</td>\n",
       "      <td>0.364662</td>\n",
       "      <td>0.237925</td>\n",
       "      <td>0.240359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  36638_at  39318_at  38514_at  266_s_at  38585_at  41266_at  \\\n",
       "0        01005  0.583368  0.535258  0.642984  0.891901  0.269871  0.605566   \n",
       "1        01010  0.505321  0.704177  0.913612  0.657634  0.402911  0.429698   \n",
       "2        03002  0.375805  0.073716  0.707562  0.847162  0.792428  0.819212   \n",
       "3        04006  1.000000  0.226960  0.119596  0.394317  0.115411  0.050117   \n",
       "4        04007  0.890125  0.631314  0.518785  0.880312  1.000000  0.858408   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "123      56007  0.072759  0.068235  0.473041  0.239455  0.223252  0.280791   \n",
       "124      64005  0.053972  0.086147  0.000065  0.136057  0.488365  0.470852   \n",
       "125      65003  0.136163  0.082575  0.456977  0.120866  0.216307  0.239017   \n",
       "126      83001  0.022913  0.026321  0.774337  0.135344  0.379465  0.544207   \n",
       "127       LAL4  0.060686  0.091866  0.710210  0.218240  0.302652  0.624061   \n",
       "\n",
       "     36108_at  39389_at  31525_s_at  ...  37655_at  1440_s_at  32260_at  \\\n",
       "0    0.069070  0.938101    0.314737  ...  0.884590   0.399443  0.379796   \n",
       "1    0.803187  0.360471    0.718665  ...  0.086277   0.432789  0.431440   \n",
       "2    0.644334  0.735292    0.828776  ...  0.750758   0.305592  0.444996   \n",
       "3    0.440698  0.649291    0.498758  ...  0.090901   0.365967  0.170796   \n",
       "4    0.638519  0.933899    0.963169  ...  0.340640   0.285412  0.561193   \n",
       "..        ...       ...         ...  ...       ...        ...       ...   \n",
       "123  0.082024  0.133053    0.768916  ...  0.371955   0.343000  0.415382   \n",
       "124  0.053909  0.116930    0.861130  ...  0.359889   0.557229  0.388962   \n",
       "125  0.069328  0.164105    0.565675  ...  0.720453   0.632587  0.652855   \n",
       "126  0.049219  0.015351    0.724584  ...  0.768953   0.357137  0.396453   \n",
       "127  0.251337  0.037572    0.452716  ...  0.528329   0.787925  0.278878   \n",
       "\n",
       "     40070_at  1056_s_at  39200_s_at  36105_at  32578_at  39383_at  33718_at  \n",
       "0    0.272379   0.216088    0.355740  0.167203  0.438651  0.395720  0.273593  \n",
       "1    0.461242   0.443493    0.291054  0.102883  0.584035  0.589994  0.401375  \n",
       "2    0.293573   0.318939    0.259713  0.271869  0.556874  0.931118  0.301529  \n",
       "3    0.565408   0.456499    0.407845  0.100038  0.670769  0.541601  0.659517  \n",
       "4    0.470224   0.468927    0.243284  0.467118  0.571348  0.425701  0.196066  \n",
       "..        ...        ...         ...       ...       ...       ...       ...  \n",
       "123  0.351072   0.351224    0.513185  0.136617  0.558943  0.225453  0.291071  \n",
       "124  0.375595   0.298626    0.584230  0.079315  0.353649  0.185538  0.461440  \n",
       "125  0.336831   0.153023    0.757112  0.179819  0.494060  0.277194  0.224542  \n",
       "126  0.674790   0.207006    0.287185  0.063400  0.327525  0.157277  0.337916  \n",
       "127  0.231955   0.233604    0.869236  0.174480  0.364662  0.237925  0.240359  \n",
       "\n",
       "[128 rows x 5001 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path= \"data.csv\"\n",
    "data_f=pd.read_csv(path)\n",
    "data_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7658ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nimfa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-306dec2be5f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnimfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nimfa'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nimfa\n",
    "\n",
    "V = np.random.rand(23, 200)\n",
    "\n",
    "# Factorization will be run 3 times (n_run) and factors will be tracked for computing\n",
    "# cophenetic correlation. Note increased time and space complexity\n",
    "bmf = nimfa.Bmf(V, max_iter=10, rank=30, n_run=3, track_factor=True)\n",
    "bmf_fit = bmf()\n",
    "\n",
    "print('K-L divergence: %5.3f' % bmf_fit.distance(metric='kl'))\n",
    "\n",
    "sm = bmf_fit.summary()\n",
    "print('Rss: %5.3f' % sm['rss'])\n",
    "print('Evar: %5.3f' % sm['evar'])\n",
    "print('Iterations: %d' % sm['n_iter'])\n",
    "print('Cophenetic correlation: %5.3f' % sm['cophenetic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf9db6",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7fab",
   "metadata": {},
   "source": [
    "3)) Inform yourself about sparse NMF (sNMF) and Non-Negative Matrix Tri-\n",
    "Factorization (NMTF)\n",
    "\n",
    "a. What is the primary difference between NMF, sNMF and NMTF? (2 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def5369",
   "metadata": {},
   "source": [
    "NMF decomposes a nonnegative matrix into the product of two matrices (W, H).\n",
    "\n",
    "sNMF additionally decomposes a nonnegative matrix into the product of two matrices (W, H) and for each cluster, it results into sparse set of features. \n",
    "\n",
    "NMTF decomposes a nonnegative matrix into the product of three matrices (W, S, H). \n",
    "\n",
    "NMF do not take into account for training labels and unlabeled test data. NMTF is more flexible and not limited for data residing in one space for tranforming training examples to test examples. So it is good for non squared data and provides more degrees of freedom than NMF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee9b3e",
   "metadata": {},
   "source": [
    "b. Which amongst the 3 NMF would be suitable for the dataset mentioned in question 1? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009718a",
   "metadata": {},
   "source": [
    "dataset consists of more than 50,000 columns which means it has a big data. NMTF uses three factorization, therefore NMTF will outperform NMF and sNMF average AC results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fad1cd",
   "metadata": {},
   "source": [
    "c. Briefly explain the reasoning behind your suggestion in question 3b.\n",
    "(2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ab133",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5df51036",
   "metadata": {},
   "source": [
    "4)) PCA and NMF are both matrix factorization methods. Write down the corresponding formulas and compare them. What is similar, what is mathematically different? Describe a situation where NMF is favored over PCA. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7d9a4",
   "metadata": {},
   "source": [
    "formulas;\n",
    "\n",
    "PCA;\n",
    "\n",
    "mean $\\sum = \\frac{1}{n-1} \\sum_{i}^{n} x_i*x_i^{T}$\n",
    "\n",
    "variance $\\frac{1}{n-1} \\sum_{i}^{n} (x_i^{T}*v)^2 = \\frac{1}{n-1} \\sum_{i}^{n} \\widetilde{\\phi}^{2}$\n",
    "\n",
    "\n",
    "NMF:\n",
    "\n",
    "D(X||WH) = $\\sum_{i, j}(x_{ij} log \\frac{x_{ij}}{w_{ij}h_{ij}} - x_{ij} + w_{ij}*h_{ij})$\n",
    "\n",
    "$w_{ik} = w_{ik} \\frac{\\sum_{j=1}^{p} w_{kj}x_{ij} / (WH)_{ij} }{ \\sum_{j=1}^{p} h_{kj} }$\n",
    "\n",
    "$h_{kj} = h_{kj} \\frac{\\sum_{j=1}^{N} w_{ik}x_{ij} / (WH)_{ij} }{ \\sum_{j=1}^{N} w_{ik} }$\n",
    "\n",
    "\n",
    "To give an example, NMF divides a face into a set of features that you can interpret as \"nose\", \"eyes\" that you can combine to reconstruct the original image. PCA however gives \"overall\" faces sorted by how well they capture the original image. \n",
    "\n",
    "NMF decomposes a dataset matrix into its non-negative submatrix and PCA gives new data feature as a result of the combination of the existing original example. \n",
    "\n",
    "For linearly separable data and reducing large number of features, PCA is better. For image processing, word and vocabulary recognition NMF gives better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9e5a1",
   "metadata": {},
   "source": [
    "5)) Have a look at this paper: https://arxiv.org/abs/1512.07548 and explain, why k- means clustering can be understood as a matrix factorization problem as well. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f6359",
   "metadata": {},
   "source": [
    "The paper mentions about previous papers showed k-means clustering can be understood as a constrained matrix factorization problem, however it is not understood in detail. So, this paper shows mathematical proof of hard k-means clustering function can be shown as matrix factorization problem. \n",
    "\n",
    "\n",
    "$\\sum = \\frac{1}{n-1} \\sum_{i}^{n} x_i*x_i^{T}$\n",
    "\n",
    "function of hard k-means clustering;\n",
    "\n",
    "$\\sum_{i=1}^{k}\\sum_{j=1}^{n} z_{ij}||x_j - \\mu_i||^{2} = ||X - MZ||^{2} = ||X - XZ^{T}(ZZ^{T})^{-1}Z||^{2}$\n",
    "\n",
    "the objective of the paper is to prove these statements as above,\n",
    "\n",
    "in notion and preliminary, it gives information about squared Frobenius norm of a matrix as $||X||^{2} = \\sum_{j}||X||^{2} = tr[X^{T}X]$\n",
    "\n",
    "then it implements step by step derivation of the k-means clustering function and expansion of each statement,\n",
    "\n",
    "first it shows this equation; $\\sum_{i=1}^{k}\\sum_{j=1}^{n} z_{ij}||x_j - \\mu_i||^{2} $ as T1, T2 and T3.\n",
    "\n",
    "then by using notions and preliminaries; it finds T1 as $tr[X^{T}X]$,  T2 as $tr[X^{T}MZ]$ and T3 as $\\sum_{i} ||\\mu_i||^{2} n_i$ \n",
    "\n",
    "then it expands $||X - MZ||^{2}$ as T4, T5, T6\n",
    "\n",
    "T1 = T4 and T2 = T5, and need to check whether T3 = T6?\n",
    "\n",
    "writer finds out that $tr[Z^{T}M^{T}MZ]$ = $\\sum(M^{T}M)_{ii}(ZZ^{T})_{ii} = \\sum_{i} ||\\mu_i||^{2} n_i$ so T3 also equals to T6\n",
    "\n",
    "then it eliminates M as taking partial derivative of $ \\sum_{j}||X||^{2}$  with respect to M; and set the equation to 0, then it yields $ M = XZ^{T}(ZZ^{T})^{{-1}}$ that each of the k-means cluster centroids µi coincides with the mean of the corresponding Ci\n",
    "\n",
    "so k-means clustering can be understood as constrained matrix factorization problem as;\n",
    "\n",
    "$min_z||X - XZ^{T}(ZZ^{T})^{-1}Z||^{2}$ st $z_{ij} \\in {0, 1}$ and $\\sum_j z_{ij} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904aca75",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723be4c",
   "metadata": {},
   "source": [
    "### Exercise 2 - Machine Learning (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dde4b4",
   "metadata": {},
   "source": [
    "The type of machine learning (e.g. supervised learning, unsupervised learning, etc.)\n",
    "applied depends on the problem at hand. Assume that we have an Alzheimer's disease\n",
    "(AD) dataset where rows represent 500 participants and columns represent 100 different\n",
    "collected measurements (such as patient characteristics, MRI measurements, and\n",
    "cognitive tests) for each participant. We are provided with diagnoses (healthy, and AD)\n",
    "of participants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a5356",
   "metadata": {},
   "source": [
    "#### 1) You are asked as a data scientist to predict the diagnosis status of 20 participants based on a model trained with data from 500 participants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0379528b",
   "metadata": {},
   "source": [
    "_**a) What type of machine learning (ML) would you choose for this task?\n",
    "(1 point)**_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e27e8",
   "metadata": {},
   "source": [
    "I would choose a supervised learning (predictive modeling), because we can train the model with fully labeled samples and the main goal is to predict the diagnosis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb4f2d",
   "metadata": {},
   "source": [
    "_**b)What are the steps you should consider before training your ML algorithm?\n",
    "Hint: List the potential preprocessing steps you would carry out. (2 points)**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b3beb",
   "metadata": {},
   "source": [
    "Before training the ML algorithm we should clean and preprocess the data.\n",
    "\n",
    "- Load the AD dataset into pandas.\n",
    "- Search for missing values. If there are, we have to decide if we drop the rows or we fill them by imputation (replacing them with their values, such as the median). I would say we could drop them if they are few because we have a dataset of 500 patients, but if they are a large number we should fill them. \n",
    "- Perform feature selection, for example by correlation coefficient. This way we use only relevant data and we get rid of noise. \n",
    "- Splitting the data into training and testing sets. And then the training set is divided again in training and validation set (only the training set is going to be used to train the model).\n",
    "- Cross-validation to figure out the best parameters for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11918a5",
   "metadata": {},
   "source": [
    "#### 2) Assuming that we do not have any information about the diagnosis (i.e. no label) of participants, answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39492c2",
   "metadata": {},
   "source": [
    "_**a) What type of machine learning would you use to group the participants\n",
    "based on the collected measurements? (1 point)**_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c7254",
   "metadata": {},
   "source": [
    "If we are learning from a dataset without labels, we will use an unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f323187",
   "metadata": {},
   "source": [
    "_**b) Based on your answer in part (2a), suggest a model you would use to carry\n",
    "on with this task, and please explain how you can determine the number of\n",
    "groups that separates your participants? (2 points)**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985bcfa",
   "metadata": {},
   "source": [
    "For an unsupervised learning we could use clustering, this way data would be groupped into clusters depending on the presence or absence of commonalities between objects. We expect a significant difference between the data of healthy and AD patients so that the model can work properly. We could use k-means or k-nearest neighbours...\n",
    "\n",
    "\n",
    "To determine the number of groups that separate the participants, in this case the number of clusters, there are some techniques that can help with that. One option to find the optimal number of clusters is to calculate the BIC, other option is the silhoutte index, etc. All these methods are going to give us the number of clusters in which the data fits best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8311d7b9",
   "metadata": {},
   "source": [
    "#### 3) Imagine that the shape of our dataset is (100, 1000), mention one pre-processing step that you would take to carry out the tasks (1) and (2)? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29f254",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0951f285",
   "metadata": {},
   "source": [
    "#### 4) You are asked to investigate the age distribution of healthy versus AD participants, name a visualization plot that can be used in this case, and explain why you think this plot would work best. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f5459d",
   "metadata": {},
   "source": [
    "I think a bar histogram would work best. An histogram allows us to represent the distribution and relationship of a single variable (age) over a set of categories (age-group). Then if we want to relate that to the distrubution of healthy vs AD, we could do a bar chart so we see the distribution of age and diagnosis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971afe3f",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
