{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c8698e",
   "metadata": {},
   "source": [
    "## Exercise sheet 8\n",
    "\n",
    "Cüneyt Erem\n",
    "3277992\n",
    "s6cuerem@uni-bonn.de\n",
    "\n",
    "Nkeh Victor Ndiwago\n",
    "3504121\n",
    "s0vinkeh@uni-bonn.de\n",
    "\n",
    "Paula Romero Jiménez\n",
    "3320220\n",
    "s0parome@uni-bonn.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc6e56c",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e347f",
   "metadata": {},
   "source": [
    "Exercise 1\n",
    "\n",
    "Decision Trees (4P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c691e21",
   "metadata": {},
   "source": [
    "1) What does the Gini index measure? Calculate the Gini index (without coding) for all three cases. (2P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf15ea",
   "metadata": {},
   "source": [
    " Gini index also known as Gini impurity measures the degree or probability of a particular variable being wrongly classified when it is chosen at random. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3280356",
   "metadata": {},
   "source": [
    " Gini index run1=[1*(1-1) +0*(1-0)]*0.5 + [0*(1-0) + 1*(1-1)]*0.5 = 0 \n",
    " \n",
    " Gini index run2=[0.5*(1-0.5) + 0.5*(1-0.5)]*0.5 + [0.5*(1-0.5) + 0.5*(1-0.5)]*0.5 = 0.5\n",
    " \n",
    " \n",
    " Gini index run3=[1*(1-1) + 0*(1-0)]*0.5 + [1*(1-1) + 0*(1-0)]*0.5 = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef80437",
   "metadata": {},
   "source": [
    "2. Explain the advantages and disadvantages of Decision Trees. What is a strategy to overcome the limitations? (2P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f3793",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "    \n",
    "   .clear visualization and interpretation:The algorithm is easy to understand,interpret and visualize as the idea is mostly used in our daily lives.\n",
    "   \n",
    "   .It can be used for both classification and regression problems.\n",
    "   \n",
    "   . It can handle both continuous and categorical variables.\n",
    "   \n",
    "   . No feature scaling is required(Standardization and Normalization):it uses the rule based approach instead of calculation.\n",
    "   \n",
    "   . decision trees automatically handle missing values and outliers.\n",
    "   \n",
    "Disadvantages:\n",
    "   \n",
    "   .Overfitting: This is the main problem of the decision tree and as a result leads to wrong predictions.inorder to fit the data(even noisy data), it keeps generating new nodesand as a result the tree becomes too complex to interpret, there by loosing its generalization ability.consequently it will perform very well on the trained data but starts making alot of mistakes on the unseen data.\n",
    "   \n",
    "   .Not suitable for large datasets:If data size is large, then one single tree may grow complex and lead to overfitting. in such situations Random Frest is recommended .\n",
    "  \n",
    " Strategey to overcome limitation:\n",
    " \n",
    "  .Inorder to overcome the limitations of tyhe decision tree tree, we use Random Forest which does not rely on a single tree. It creates a Forest of trees and takes the decision based on vote count. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80804de8",
   "metadata": {},
   "source": [
    "Exercise 2\n",
    "\n",
    "Random Forests and Boosting (10P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cb0c0",
   "metadata": {},
   "source": [
    "1. Explain in your own words, why random forests yield a variance reduction compared to a single  decision tree. (2P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b47111c",
   "metadata": {},
   "source": [
    "Random Forest uses fully grown decision trees with low bias and high variance. It reduces error by reducing variance. the trees are made uncorrelated to maximise  decrease in variance. in other words Bagging(averaging predictions from lots of trees) and Random Forest use these high variance models and aggregate them in order to reduce variance and enhance prediction accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f54a4",
   "metadata": {},
   "source": [
    "3) Below is a table to contrast three different methods. Replace the “?” in the Variance/Bias columns with “high” or “low” and give statistical explanations on each decision. (3P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e77dd0",
   "metadata": {},
   "source": [
    "| Method     | Variance | Bias | Explanation |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| A single decision tree | high | low | A decision tree has low bias but high variance because it can easily change as small change in input variable.it does not generalize the pattern well. it works well only for the training data but leads to overfitting|\n",
    "| A single decision tree vs. random forest | high vs. low | low vs. high |Bagging and Random Forest use these high variance models and aggregate them in order to reduce variance and enhance prediction accuracy. both Bagging and random forest use Bootstrap sampling which increases bias in the single tree.also the random forest limits the allowed variables to split at each node there by increasing bias for single random forest trees.to conclude if the increase in bias of of the single trees in bagging and RF is not outweighing the variance reduction then the prediction accuracy will increase. |\n",
    "| A single decision tree vs. (gradient) boosted decision trees | high vs. low | high vs low | boosting reduces error by reducing bias and also Variance is reduced in gradient boosted decision trees by simply aggregating the output from many models. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edfb40",
   "metadata": {},
   "source": [
    "4. Differences between Random Forests and gradient boosting: (2P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a26fd",
   "metadata": {},
   "source": [
    ". In gradient boosting, training is done sequentially one tree at a time each new tree is built to improve on or correct the errors of the previous one while in Random Forest each decision treee is build and calculated independently.\n",
    "\n",
    ".In Random Forest, the results of the decision trees are agreegated at the end of the process.while in Gradient Boosting, the results of each decision tree  are aggregated along the way to calculate the final result at the end of the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e052d8",
   "metadata": {},
   "source": [
    " Which approach usually trains faster? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897890e1",
   "metadata": {},
   "source": [
    "Random Forest trains faster than gradient boosting,because it works only on a subset of features so it can easily work with hundreds of features .it is worth noting that Random Forest is good for high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2e16c",
   "metadata": {},
   "source": [
    " Which one is more prone to overfitting? Why?\n",
    " \n",
    "  Gradient Boosting is more prone to overfitting,because when the depth of trees increases,the model is likely going to overfit the training data.Also increasing the number of gradient boosting iterations increases overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334dcc3",
   "metadata": {},
   "source": [
    "5. True or False for tree-based methods? Explain your answer. (3P) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb417959",
   "metadata": {},
   "source": [
    ".Features need to be standardized?\n",
    "\n",
    "   False, because they are not sensitive to Variance\n",
    "   \n",
    ".Only suited for continuous variables?\n",
    "\n",
    "  False, Tree-based methods are suited both for categorical variables and continuous variables.the problem with continuous variables is that they can have more than one value, or a spectrum of values, and as a result in predictive analysis, calculations calcul;ations rapidly grow big and complex.\n",
    "  \n",
    " .Deeper trees yield better ensemble models?\n",
    " \n",
    "  True, because deeper trees reduces bias though another problem of large variance arises which is overcomed by Bagging(Bootstrap Aggregation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9393632",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630ef6b",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "### Programming Task – Random Forest (8P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5342fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95723a48",
   "metadata": {},
   "source": [
    "**1. Load the provided Cleveland subset onto your Notebook (file name: processed.cleveland_cleaned.data). Note that this dataset is prepared explicitly for this exercise and might differ from the original version. (1P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0628b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       "\n",
       "   slope   ca  thal  num  \n",
       "0    3.0  0.0   6.0    0  \n",
       "1    2.0  3.0   3.0    2  \n",
       "2    2.0  2.0   7.0    1  \n",
       "3    3.0  0.0   3.0    0  \n",
       "4    1.0  0.0   3.0    0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('processed.cleveland_cleaned.data')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32b8a0",
   "metadata": {},
   "source": [
    "**2. Make the column “num” as the target variable and the rest of the columns as the input variables. Split the dataset into 80% train and 20% test sets using the Sklearn train_test_split() function. Set random_state for reproducibility. (1P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73451183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataframe into target and rest\n",
    "features = dataset.drop(['num'], axis=1) \n",
    "target = dataset['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e750f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide the dataset into 80% training and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007f959",
   "metadata": {},
   "source": [
    "**3. Define a random forest classifier. Define a 3-fold cross-validation for the dataset using the Sklearn KFold() function. Remember to enable order shuffling of the dataset. (1P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c02cedcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the random forest classifier.\n",
    "clf = RandomForestClassifier(random_state=6)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8eacddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "y_pred=clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5105c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the KFold cross validation\n",
    "cv = KFold(n_splits=3, random_state=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953a8d5",
   "metadata": {},
   "source": [
    "**4. Perform grid search hyperparameter optimization on the random forest classifier using the training set with these hyperparameters ranges:**\n",
    "- a. Number of estimators: 50, 100, 200, 500, 1000\n",
    "- b. Criterion: gini, entropy\n",
    "- c. Maximum tree depth: None, 10, 50\n",
    "\n",
    "**Supply the grid search function with the defined estimator, hyperparameter ranges, and folds. Make “accuracy” as the scoring function. What is the best value for each hyperparameter? What is the best accuracy score obtained? (1P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d50d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid  \n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 50],\n",
    "    'n_estimators': [50, 100, 200, 500, 1000],\n",
    "    'criterion': ['gini','entropy']\n",
    "}\n",
    "\n",
    "# Define the grid search CV\n",
    "search = GridSearchCV(estimator = clf, param_grid = param_grid, cv = cv, n_jobs = -1, verbose = 2, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f932ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "# Execute the search\n",
    "result = search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1c3ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.5822784810126582\n",
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Best value for each hyperparameter and best accuracy score\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ff223",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd51546",
   "metadata": {},
   "source": [
    "5. Taking the best values of hyperparameters from (4), get the cross-validation score of the random forest classifier using the Sklearn cross_val_score() function. What is the mean crossvalidated accuracy score? (1P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acd7b5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.570 (0.037)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "scores = cross_val_score(result, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c1a3d0",
   "metadata": {},
   "source": [
    "6. Evaluate the trained model with test set using the Sklearn classification_report() function. Explain the difference between the accuracy and the f1-score. (1P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf9d318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88        36\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.22      0.22      0.22         9\n",
      "           3       0.00      0.00      0.00         5\n",
      "           4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.63        60\n",
      "   macro avg       0.20      0.24      0.22        60\n",
      "weighted avg       0.50      0.63      0.56        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuneyt/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/cuneyt/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/cuneyt/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6830a37",
   "metadata": {},
   "source": [
    "accuracy is 0.63, highest f1-score is element 0 with 0.88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51ccb51",
   "metadata": {},
   "source": [
    "F1 Score is the weighted average of Precision and Recall so it considers false positives and false negatives. Accuracy calculates a ratio of correctly predicted observation to the total observations. Therefore F1 is more useful than accuracy\n",
    "\n",
    "Accuracy = TP+TN/TP+FP+FN+TN\n",
    "\n",
    "Precision = TP/TP+FP\n",
    "\n",
    "Recall = TP/TP+FN\n",
    "\n",
    "F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6aa6d0",
   "metadata": {},
   "source": [
    "7. Obtain the feature importance of the model using the .feature_importance_ method of the model itself. Which feature contributes the most to the classification? (1P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f179e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10895828 0.02794171 0.06855011 0.09099511 0.1130698  0.01322721\n",
      " 0.0326378  0.14667175 0.04251345 0.11917888 0.04691327 0.0995931\n",
      " 0.08974954] \n",
      "\n",
      "[0.03524257 0.02085097 0.03828257 0.03193676 0.03931946 0.01229494\n",
      " 0.01772846 0.05045822 0.0294863  0.04239955 0.02785394 0.03783187\n",
      " 0.0466883 ]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "mean = np.mean([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "print(mean, '\\n')\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d50a936b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "forest_importances = pd.Series(importances, index=features.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"MDI feature importances\")\n",
    "ax.set_ylabel(\"Mean\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e39a3",
   "metadata": {},
   "source": [
    "thalach has most higher importance value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703eb60",
   "metadata": {},
   "source": [
    "8. Obtain feature importance of the model using the Sklearn permutation_importance() function. Set “repeat” to 10 times. Display the mean and standard deviation of each feature’s importance value. Do you notice any difference between the outcome of (7) and (8)? (1P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4cb836d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03881857 0.01097046 0.05400844 0.01012658 0.01687764 0.00421941\n",
      " 0.         0.05738397 0.02109705 0.04978903 0.02151899 0.08691983\n",
      " 0.08649789]\n",
      "\n",
      "[0.00901019 0.00506329 0.01144697 0.00206708 0.00730823 0.\n",
      " 0.         0.00965867 0.00680359 0.00725935 0.00295359 0.00782584\n",
      " 0.01182188]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(clf, X_train, y_train, n_repeats=10, random_state=0)\n",
    "\n",
    "print(result.importances_mean)\n",
    "print()\n",
    "print(result.importances_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97842f10",
   "metadata": {},
   "source": [
    "mean and std values are more lower ın (8) than in (7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d40b6b",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
